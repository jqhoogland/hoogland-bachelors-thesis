\documentclass[12pt]{report}
\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc} % use 8-bit T1 fonts
\usepackage{fancyref} % \fref \Fref
\usepackage{hyperref} % hyperlinks
\usepackage{url} % simple URL typesetting
\usepackage{booktabs} % professional-quality tables
\usepackage{amsfonts} % blackboard math symbols
\usepackage{mathtools}
\usepackage{nicefrac} % compact symbols for 1/2, etc.
\usepackage{microtype} % microtypography
\usepackage{lipsum}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor} % for marking points of confusion
\usepackage{graphicx}
\usepackage{natbib}

% Referencing

% \numberwithin{equation}{section}
% \numberwithin{figure}{section}
% \numberwithin{table}{section}

\renewcommand{\fancyrefdefaultformat}{plain}
%\newcommand(\fref)[1]{\cref{#1}} % Because I started with fancyref
\newcommand{\pref}[1]{(\fref{#1})}
\newcommand{\sref}[1]{(see
  \fref{#1})}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bolds}[1]{\boldsymbol{#1}}

% \DeclarePairedDelimiter\abs{\lvert}{\rvert}%

%1 Exectation value
\newcommand{\expect}[1]{\langle{}{#1}\rangle{}}

% Collections of samples
\newcommand{\mcH}{\mathcal{H}}
\newcommand{\mcE}{\mathcal{E}}
\newcommand{\mcB}{\mathcal{B}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcS}{\mathcal{S}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\mcR}{\mathcal{R}}


% Microstates (bold lowercase)
\newcommand{\bh}{\bolds{h}}
\newcommand{\be}{\bolds{e}}
\newcommand{\bb}{\bolds{b}}
\newcommand{\bv}{\bolds{v}}
\newcommand{\bx}{\bolds{x}}
\newcommand{\by}{\bolds{y}}
\newcommand{\bs}{\bolds{s}}
\newcommand{\bo}{\bolds{o}}
\newcommand{\br}{\bolds{r}}

% Microstates (bold lowercase)
\newcommand{\bH}{\bolds{H}}
\newcommand{\bE}{\bolds{E}}
\newcommand{\bB}{\bolds{B}}
\newcommand{\bV}{\bolds{V}}
\newcommand{\bX}{\bolds{X}}
\newcommand{\bY}{\bolds{Y}}
\newcommand{\bS}{\bolds{S}}
\newcommand{\bT}{\bolds{T}}

% microstates explicit (bold lowercase)
\newcommand{\seth}{\{h_j\}}
\newcommand{\sete}{\{e\}}
\newcommand{\setb}{\{b\}}
\newcommand{\setv}{\{v_i\}}
\newcommand{\setx}{\{x_i\}}
\newcommand{\sety}{\{y_j\}}
\newcommand{\sets}{\{s_i\}}
\newcommand{\setr}{\{r_j\}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator{\sgn}{sgn}

% Greek letters
\renewcommand{\l}{\lambda}
\renewcommand{\b}{\beta}
\renewcommand{\L}{\Lambda}
\renewcommand{\k}{\kappa}
\newcommand{\T}{\Theta}
\renewcommand{\P}{\Psi}
\newcommand{\coloneq}{\mathrel{\resizebox{\widthof{$\mathord{=}$}}{\height}{ $\!\!=\!\!\resizebox{1.2\width}{0.8\height}{\raisebox{0.23ex}{$\mathop{:}$}}\!\!$ }}}
\newcommand{\eqcolon}{\mathrel{\resizebox{\widthof{$\mathord{=}$}}{\height}{ $\!\!\resizebox{1.2\width}{0.8\height}{\raisebox{0.23ex}{$\mathop{:}$}}\!\!=\!\!$ }}}


\title{Restricted Boltzmann Machines and the Renormalization Group:
  Learning Relevant Information in Statistical Physics}


\author{%
  Jesse Q\@. Hoogland \\
  Amsterdam University College\\
  Amsterdam, the Netherlands \\
  \texttt{jessequinten@gmail.com} \\
  \AND{}
  Dr\@. P\@. Marcos Crichigno \\
  Supervisor \\
  University of Amsterdam\\
  Amsterdam, the Netherlands \\
  \texttt{P.M.Crichigno@uva.nl} \\
  \And{}
  Prof\@. Dr\@. Max Welling \\
  Reader \\
  University of Amsterdam\\
  Amsterdam, the Netherlands\\
  \texttt{M.Welling@uva.nl}\\
  \And{}
  Dr\@. Michael P\@. McAssey \\
  Tutor \\
  Amsterdam University College\\
  Amsterdam, the Netherlands\\
  \texttt{M.P.McAssey@auc.nl} \\
}

\begin{document}
\maketitle
\begin{abstract}
  Recent work has drawn attention to the links between statistical
  physics and machine learning (ML) and, in particular, to comparisons
  between the renormalization group (RG) and deep neural networks,
  respectively. These have inspired renewed interest in the
  information-theoretic framework underpinning these fields, prompting
  a better understanding of what RG is. In this
  capstone, we introduce and expand upon these connections from the
  ground up. Starting with the basics of ML and RG, we work our way to
  an algorithm implementated on neural networks that learns optimal,
  model-independent RG procedures, the real-space mutual information
  (RSMI) algorithm. Along the way, we review the current state of the
  literature, clarifying misconceptions in earlier works.  With the
  RSMI algorithm, we review a novel calculation of the Ising model
  critical exponent, and we generalize this approach to arbitrary
  lattice systems.  We release an open-source library, \textit{rgpy},
  for implementing these novel procedures, and close with a discussion
  of the wide-ranging implications.
\end{abstract}
%
\keywords{Machine Learning, Restricted Boltzmann Machines, The
  Renormalization Group, Information Theory, Mutual Information}
%
\tableofcontents

% SECTION 1: INTRODUCTION
\input{introduction}

% SECTION 2: ELEMENTS OF STATISTICAL MECHANICS
\input{sm}

% SECTION 3: MACHINE LEARNING
\input{ml}

\input{comparison}

% SECTION 4: RSMI ALGORITHM
\input{rsmi}

% SECTION 5: VALIDATION OF RSMI ALGORITHM & EXTENSION
\input{validation}

% SECTION 6: DISCUSSION
\input{discussion}

\input{acknowledgements}

% APPENDIX
\input{appendix}

\bibliographystyle{unsrt} \bibliography{references}


\end{document}
